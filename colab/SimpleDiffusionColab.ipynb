{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hizibu7/505-jeok/blob/main/colab/SimpleDiffusionColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2022 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "WzMr0D9Y9DLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Diffusion\n",
        "\n",
        "This colab is a demonstration of the code and principles behind the [Variational Diffusion Models paper](https://arxiv.org/abs/2107.00630) and serves as a standalone implementation to complement the [open source release](https://github.com/google-research/vdm).\n",
        "\n",
        "The goal will be to have a minimalistic, easy to understand implementation of a diffusion model, most closely following the [VDM Paper](https://arxiv.org/abs/2107.00630)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb)\n"
      ],
      "metadata": {
        "id": "QlxZLC4w084z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preamble"
      ],
      "metadata": {
        "id": "yXz_Q2kr6eyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports\n",
        "\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax import random, vmap, grad, jit\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import einops\n",
        "from tqdm import tqdm, trange\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "from scipy.stats import describe\n",
        "import functools\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "import tensorflow_datasets as tfds\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "from clu import deterministic_data"
      ],
      "metadata": {
        "id": "fpI_nD271CyV",
        "outputId": "92af623b-16b7-44b3-ed1f-9ca3b5e2997d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'clu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-27febfd8e570>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeterministic_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clu'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils\n",
        "# Various helper utility functions.\n",
        "\n",
        "import io\n",
        "import math\n",
        "from IPython.display import display_png\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def imify(arr, vmin=None, vmax=None, cmap=None, origin=None):\n",
        "  \"\"\"Convert an array to an image.\n",
        "\n",
        "  Arguments:\n",
        "    arr : array-like The image data. The shape can be one of MxN (luminance),\n",
        "      MxNx3 (RGB) or MxNx4 (RGBA).\n",
        "    vmin : scalar, optional lower value.\n",
        "    vmax : scalar, optional *vmin* and *vmax* set the color scaling for the\n",
        "      image by fixing the values that map to the colormap color limits. If\n",
        "      either *vmin* or *vmax* is None, that limit is determined from the *arr*\n",
        "      min/max value.\n",
        "    cmap : str or `~matplotlib.colors.Colormap`, optional A Colormap instance or\n",
        "      registered colormap name. The colormap maps scalar data to colors. It is\n",
        "      ignored for RGB(A) data.\n",
        "        Defaults to :rc:`image.cmap` ('viridis').\n",
        "    origin : {'upper', 'lower'}, optional Indicates whether the ``(0, 0)`` index\n",
        "      of the array is in the upper\n",
        "        left or lower left corner of the axes.  Defaults to :rc:`image.origin`\n",
        "          ('upper').\n",
        "\n",
        "  Returns:\n",
        "    A uint8 image array.\n",
        "  \"\"\"\n",
        "  sm = cm.ScalarMappable(cmap=cmap)\n",
        "  sm.set_clim(vmin, vmax)\n",
        "  if origin is None:\n",
        "    origin = mpl.rcParams[\"image.origin\"]\n",
        "  if origin == \"lower\":\n",
        "    arr = arr[::-1]\n",
        "  rgba = sm.to_rgba(arr, bytes=True)\n",
        "  return rgba\n",
        "\n",
        "def rawarrview(array, **kwargs):\n",
        "  \"\"\"Visualize an array as if it was an image in colab notebooks.\n",
        "\n",
        "  Arguments:\n",
        "    array: an array which will be turned into an image.\n",
        "    **kwargs: Additional keyword arguments passed to imify.\n",
        "  \"\"\"\n",
        "  f = io.BytesIO()\n",
        "  imarray = imify(array, **kwargs)\n",
        "  plt.imsave(f, imarray, format=\"png\")\n",
        "  f.seek(0)\n",
        "  dat = f.read()\n",
        "  f.close()\n",
        "  display_png(dat, raw=True)\n",
        "\n",
        "\n",
        "def reshape_image_batch(array, cut=None, rows=None, axis=0):\n",
        "  \"\"\"Given an array of shape [n, x, y, ...] reshape it to create an image field.\n",
        "\n",
        "  Arguments:\n",
        "    array: The array to reshape.\n",
        "    cut: Optional cut on the number of images to view. Will default to whole\n",
        "      array.\n",
        "    rows: Number of rows to use.  Will default to the integer less than the\n",
        "      sqrt.\n",
        "    axis: Axis to interpretate at the batch dimension.  By default the image\n",
        "      dimensions immediately follow.\n",
        "\n",
        "  Returns:\n",
        "    reshaped_array: An array of shape [rows * x, cut / rows * y, ...]\n",
        "  \"\"\"\n",
        "  original_shape = array.shape\n",
        "  assert len(original_shape) >= 2, \"array must be at least 3 Dimensional.\"\n",
        "\n",
        "  if cut is None:\n",
        "    cut = original_shape[axis]\n",
        "  if rows is None:\n",
        "    rows = int(math.sqrt(cut))\n",
        "\n",
        "  cols = cut // rows\n",
        "  cut = cols * rows\n",
        "\n",
        "  leading = original_shape[:axis]\n",
        "  x_width = original_shape[axis + 1]\n",
        "  y_width = original_shape[axis + 2]\n",
        "  remaining = original_shape[axis + 3:]\n",
        "\n",
        "  array = array[:cut]\n",
        "  array = array.reshape(leading + (rows, cols, x_width, y_width) + remaining)\n",
        "  array = np.moveaxis(array, axis + 2, axis + 1)\n",
        "  array = array.reshape(leading + (rows * x_width, cols * y_width) + remaining)\n",
        "  return array\n",
        "\n",
        "def zoom(im, k, axes=(0, 1)):\n",
        "  for ax in axes:\n",
        "    im = np.repeat(im, k, ax)\n",
        "  return im\n",
        "\n",
        "\n",
        "def imgviewer(im, zoom=3, cmap='bone_r', normalize=False, **kwargs):\n",
        "  if normalize:\n",
        "    im = im - im.min()\n",
        "    im = im / im.max()\n",
        "  return rawarrview(zoom(im, zoom), cmap=cmap, **kwargs)\n",
        "\n",
        "def param_count(pytree):\n",
        "  return sum(x.size for x in jax.tree_leaves(pytree))\n",
        "\n",
        "replicate = flax.jax_utils.replicate\n",
        "unreplicate = flax.jax_utils.unreplicate"
      ],
      "metadata": {
        "id": "7BgesH_w6XVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "print(\"devices:\", jax.devices())\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "KAP7FmTdlkSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "For this example colab, we'll be using the [EMNIST dataset](https://www.nist.gov/itl/products-and-services/emnist-dataset), a collection of handwritten numbers and letters.  Think of EMNIST as a larger drop-in replacement for MNIST.\n",
        "\n",
        "To keep things simple, we will model binarized data.  To binarize the data we will treat the continuous values stored in the dataset as probabilities for independently sampling the pixels in the image."
      ],
      "metadata": {
        "id": "CHNnr0krh7qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data\n",
        "import os\n",
        "\n",
        "# MNIST로 먼저 시도\n",
        "# MNIST와 Fashion MNIST 데이터셋 로드\n",
        "mnist_builder = tfds.builder('mnist')\n",
        "fashion_mnist_builder = tfds.builder('fashion_mnist')\n",
        "\n",
        "mnist_builder.download_and_prepare()\n",
        "fashion_mnist_builder.download_and_prepare()\n",
        "\n",
        "train_split = tfds.split_for_jax_process('train', drop_remainder=True)\n",
        "test_split = tfds.split_for_jax_process('test', drop_remainder=True)\n",
        "\n",
        "\n",
        "def mix_datasets(example_mnist, example_fashion, p=0.2):\n",
        "    # 20% 확률로 Fashion MNIST 이미지 선택\n",
        "    random_val = tf.random.uniform(shape=[])\n",
        "\n",
        "    image = tf.cond(\n",
        "        random_val < p,\n",
        "        lambda: example_fashion['image'],\n",
        "        lambda: example_mnist['image']\n",
        "    )\n",
        "\n",
        "    # MNIST 레이블은 그대로 유지 (이상치 탐지를 위해)\n",
        "    label = example_mnist['label']\n",
        "\n",
        "    # 이미지 전처리\n",
        "    image = tf.cast(image, 'float32')\n",
        "    image = tf.random.uniform(image.shape) < image / 255.0\n",
        "\n",
        "    return (image, label)\n",
        "\n",
        "\n",
        "def preprocess_fn(example):\n",
        "    image = tf.cast(example['image'], 'float32')\n",
        "    image = tf.random.uniform(image.shape) < image / 255.0\n",
        "    return (image, example[\"label\"])\n",
        "\n",
        "def unconditional_fraample\n",
        "    label = tf.where(tf.random.uniform(label.shape) > p, label, 0)\n",
        "    return (image, label)\n",
        "\n",
        "batch_size = 4 * 128 * jax.device_count()\n",
        "\n",
        "\n",
        "\n",
        "# 두 데이터셋을 조합\n",
        "train_ds = deterministic_data.create_dataset(\n",
        "    mnist_builder,\n",
        "    fashion_mnist_builder,\n",
        "    split=train_split,\n",
        "    rng=jax.random.PRNGKey(0),\n",
        "    shuffle_buffer_size=100,\n",
        "    batch_dims=[jax.local_device_count(), batch_size // jax.device_count()],\n",
        "    num_epochs=None,\n",
        "    preprocess_fn=mix_datasets,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 테스트셋은 순수 MNIST만 사용\n",
        "test_ds = deterministic_data.create_dataset(\n",
        "    mnist_builder,\n",
        "    split=test_split,\n",
        "    rng=jax.random.PRNGKey(0),\n",
        "    batch_dims=[jax.local_device_count(), batch_size // jax.device_count()],\n",
        "    num_epochs=1,\n",
        "    preprocess_fn=preprocess_fn\n",
        ")\n",
        "\n",
        "\n",
        "def create_input_iter(ds):\n",
        "    def _prepare(xs):\n",
        "        def _f(x):\n",
        "            x = x._numpy()\n",
        "            return x\n",
        "        return jax.tree_util.tree_map(_f, xs)\n",
        "    it = map(_prepare, ds)\n",
        "    it = flax.jax_utils.prefetch_to_device(it, 2)\n",
        "    return itction(example, p=0.2):\n",
        "    image, label = ex"
      ],
      "metadata": {
        "id": "X8IScQxxlPZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = next(create_input_iter(train_ds))\n",
        "rawarrview(reshape_image_batch(out[0][0].squeeze()), cmap='bone_r')"
      ],
      "metadata": {
        "id": "zuFSX-UAsx3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "There are some [great](https://yang-song.net/blog/2021/score/) [introductions](https://huggingface.co/blog/annotated-diffusion) to diffusion and score-based models. Here we're going to focus on the interpretation of these models in [Variational Diffusion Models (VDM)](https://arxiv.org/abs/2107.00630) where diffusion models are thought of as a hierarchical latent variable model like a [Variational Autoencoder (VAE)](https://arxiv.org/abs/1312.6114).\n",
        "\n",
        "The KL divergence is non-negative, which means that the KL between two joint distributions upper bounds the KL of their marginals:\n",
        "$$ \\int dx\\, dz\\, p(x,z) \\log \\frac{p(x,z)}{q(x,z)} = \\int dx\\, p(x) \\log \\frac{p(x)}{q(x)} + \\left[ \\int dx\\, p(x) \\int dz\\, p(z|x) \\log \\frac{p(z|x)}{q(z|x)}\\right] \\\\ \\geq \\int dx\\, p(x) \\log \\frac{p(x)}{q(x)} \\geq 0 $$\n",
        "This immediately let's us generate an upper bound on the marginal likelihood of one joint in terms of another.  This can motivate the use of ELBO for training VAEs.\n",
        "\n",
        "Naturally, this trick works even if we add more variables.  Imagine extending our forward process so that it has $T$ total steps, $x \\to z_0 \\to z_1 \\to z_2 \\to \\cdots \\to z_T$.  By monotonicity, the KL between the full joints will still be an upper bound on the KL between the marginals.\n",
        "\n",
        "$$ \\int dx\\, \\prod_i dz_i\\, q(x,z_0,\\cdots,z_T) \\log \\frac{q(x) q(z_0|x) q(z_1|z_0) \\cdots q(z_T|z_{T-1})}{p(x|z_0) p(z_0|z_1)\\cdots p(z_{T-1}|z_T) p(z_T)}   \\geq \\int dx\\, q(x) \\log \\frac{q(x)}{p(x)} \\geq 0 $$\n",
        "\n",
        "If we add enough steps and try to ensure that the conditional KLs are small enough, we might hope to get a very good generative model..."
      ],
      "metadata": {
        "id": "K6ltZUUwy-cH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward process\n",
        "\n",
        "We'll choose as our forward process a simple Gaussian diffusion process, that adds increasing amounts of noise at each step.\n",
        "\n",
        "That is we'll take:\n",
        "$$ q(z_t|z_{t-1} ) = \\mathcal N(\\alpha_{t|t-1} z_{t-1} , \\sigma^2_{t|t-1} I ) $$\n",
        "Where we'll take the previous sample and scale it down and add some gaussian noise to it.  Here and in a lot of the diffusion work, the process is taken to be *variance preserving* by setting:\n",
        "$$ \\alpha^2 = 1 - \\sigma^2 $$\n",
        "\n",
        "One particuarly nice thing about using gaussians for every step of the forward process here is that the composition of a bunch of conditional gaussians is itself gaussian so we will have a closed form for the marginal distribution at any intermediate time:\n",
        "$$ q(z_t|x) = \\mathcal N(\\alpha_t x, \\sigma_t^2 I )$$\n",
        "which will itself be variance preserving:\n",
        "$$ \\alpha_t = \\sqrt{1 - \\sigma_t^2} .$$\n",
        "\n",
        "At this point, the entire forward process is characterized by the amount of noise we've added at each stage.  Another way we could discuss this is in terms of the *signal-to-noise* ratio,\n",
        "$$ \\mathsf{snr}_t = \\frac{\\alpha_t^2}{\\sigma^2_t} $$\n",
        "\n",
        "There are lots of ways to describe the same thing, but the point is that as long as we are content to restrict our attention to variance preserving gaussian forward processes, then the entire process is characterized by a single curve, a noise schedule that determines how much noise we add at each step.  In a lot of diffusion work this schedule is fixed, but in the VDM paper it is learned by means of a neural network.  To obtain an efficient parameterization of the process, we'll parameterize the noise schedule in terms of the log signal to noise ratio.\n",
        "\n",
        "$$ \\gamma(t) = \\log \\mathsf{snr}_t = \\log \\frac{\\alpha_t^2}{\\sigma_t^2} $$\n",
        "\n",
        "So we can see for instance that:\n",
        "$$ \\sigma_t^2 = \\textsf{sigmoid}(-\\gamma(t) ) $$\n",
        "(Note that this has a sign flip with respect to the VDM paper, where $\\gamma$ was the negative of the log SNR).\n",
        "\n",
        "$\\gamma(t)$ could in general be a learned neural network, but to keep things simple, in this colab we'll fix\n",
        "$$\\gamma(t) = \\gamma_{\\min} + (\\gamma_{\\max} - \\gamma_{\\min}) t $$\n",
        "for a value of $t$ that ranges from 0 to 1, so that our log snr is linear in $t$ and all we have to choose is the mininimum and maximum log snr value.\n"
      ],
      "metadata": {
        "id": "3vqZP7w43MsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma(ts, gamma_min=-6, gamma_max=6):\n",
        "  return gamma_max + (gamma_min - gamma_max) * ts\n",
        "\n",
        "def sigma2(gamma):\n",
        "  return jax.nn.sigmoid(-gamma)\n",
        "\n",
        "def alpha(gamma):\n",
        "  return np.sqrt(1 - sigma2(gamma))"
      ],
      "metadata": {
        "id": "b9UD7w0vylPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a schedule which has linear log snr, the $\\alpha$ and $\\sigma^2$ values look like sigmoids:"
      ],
      "metadata": {
        "id": "eFlduRq6pN4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = np.linspace(0, 1, 1000)\n",
        "plt.plot(ts, sigma2(gamma(ts)), label=r'$\\sigma^2$')\n",
        "plt.plot(ts, alpha(gamma(ts)), label=r'$\\alpha$')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "3YjU8WY9BfKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize what this would look like for a batch of images:"
      ],
      "metadata": {
        "id": "OdTDRD4GqdN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_preserving_map(x, gamma, eps):\n",
        "  a = alpha(gamma)\n",
        "  var = sigma2(gamma)\n",
        "  return a * x + np.sqrt(var) * eps"
      ],
      "metadata": {
        "id": "HWx6GXn8g6cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ii = 2 * out[0][0,:10].squeeze() - 1\n",
        "ii.shape\n",
        "TT = 30\n",
        "results = np.zeros((10, TT+1, 28, 28))\n",
        "results = results.at[:,0].set(ii)\n",
        "\n",
        "tt = np.linspace(0, 1, TT)\n",
        "eps = random.normal(jax.random.PRNGKey(0), (10, 28, 28))\n",
        "for i, t in enumerate(tt):\n",
        "  results = results.at[:,i+1].set(variance_preserving_map(ii, gamma(t), eps))\n",
        "rawarrview(reshape_image_batch(results.reshape((-1, 28, 28)), rows=10), cmap='bone_r')"
      ],
      "metadata": {
        "id": "LRNoO9phCI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many diffusion models perform this forward process in pixel space, but to make this colab more efficient and cheaper to train we are going to use *latent diffusion*, in that our first step $q(z_0|x)$ is going to use an encoder much like an ordinary gaussian VAE model that maps the image to some lower dimensional latent representation, and we'll then be adding gaussian noise in this latent space.  In particular if the first step gaussian is set to have a nonlinear mean:\n",
        "\n",
        "$$ q(z_0|x) = \\mathcal N ( f(x), \\sigma_0^2 I ) $$\n",
        "Then we'll still be able to compute some later step in the forward process $q(z_t|x)$ in closed form."
      ],
      "metadata": {
        "id": "gLImFup4qgXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reverse Process"
      ],
      "metadata": {
        "id": "pwsXTFNYFBkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the forward process in place, now we tackle the reverse (and our learned model).\n",
        "\n",
        "We can do a bit of math there. We've defined a whole set of encoding distribuitons of the form:\n",
        "\n",
        "$$ q(z_t | x) = \\mathcal N (\\alpha_t x, \\sigma_t^2 I) $$\n",
        "\n",
        "which means we can work out various conditionals, first notice that since everything is Gaussian\n",
        "\n",
        "$$ q(z_t | z_s) =\\mathcal N (\\alpha_{t|s} x, \\sigma_{t|s}^2 I) $$\n",
        "with\n",
        "$$\\alpha_{t|s} = \\frac{\\alpha_t}{\\alpha_s} $$\n",
        "$$ \\sigma_{t|s}^2 = \\sigma_{t}^2 - \\alpha_{t|s}^2 \\sigma_s^2 $$\n",
        "<!-- not sure the easiest way to show these results but they make sense.  \n",
        "I guess we could just reason in terms of moments.  If we have $z_s$ then to get $z_t$ we want to multiply by $\\alpha_{t|s}$ and since variances add, we can also get the variances to line up. -->\n",
        "\n",
        "So the next question becomes, what does the $q(z_s|z_t,x)$ distribution look like?\n",
        "\n",
        "We have that:\n",
        "\n",
        "$$ q(z_s|z_t,x) = \\frac{q(z_t|z_s,x) q(z_s|x)}{q(z_t|x)} = \\frac{q(z_t|z_s)q(z_s|s)}{q(z_t|x)} = \\mathcal N(\\mu_Q(z_t, x, s, t), \\sigma_Q^2(s, t) I) $$\n",
        "where (which you can get from matching moments\n",
        "\n",
        "$$ \\mu_Q(z_t, x, s, t) = \\frac{\\alpha_{t|s} \\sigma_s^2}{\\sigma_t^2} z_t + \\frac{\\alpha_s \\sigma_{t|s}^2}{\\sigma_t^2} x $$\n",
        "and\n",
        "$$ \\sigma_Q^2(s,t) = \\sigma_{t|s}^2 \\sigma_s^2 / \\sigma_t^2 $$\n",
        "\n",
        "So, in words, in our original forward process, the conditional reverse process, that is the probability of getting some $z_s$ at an earlier stage if we know both a later stage $z_t$ as well as the original image is a conditional gaussian, a sort of mixing of $z_t$ and $x$.\n",
        "\n",
        "So, in the diffusion model, we try to match things and so we choose as our reverse process something of the form:\n",
        "\n",
        "$$ p(z_t|z_s) = q(z_s | z_t, x = \\hat x_\\theta(z_t;t)) $$\n",
        "this seems to be the crucial thing we have going on here.  We are making a structural assumption about the form of the reverse process.  In the limit of infinite number of steps, this reverse process would be exact. We are going to learn the reverse process, but we are going to make use of a large amount of knowledge we have about the forward process to parameterize it intelligently.  Here we are going to learn the reverse process so that it has the form of the conditional reverse of the forward process. Writing things out, this means the mean takes the form:\n",
        "\n",
        "$$ \\begin{align}\n",
        "  \\mu_\\theta(z_t; s, t) &= \\frac{\\alpha_{t|s}\\sigma_s^2}{\\sigma_t^2} z_t + \\frac{\\alpha_s \\sigma_{t|s}^2}{\\sigma_t^2} \\hat x_\\theta(z_t; t) \\\\\n",
        "  &= \\frac{1}{\\alpha_{t|s}} z_t - \\frac{\\sigma_{t|s}^2}{\\alpha_{t|s}\\sigma_t} \\hat \\epsilon_\\theta(z_t;t) \\\\\n",
        "  &= \\frac{1}{\\alpha_{t|s}} z_t + \\frac{\\sigma_{t|s}^2}{\\alpha_{t|s}} s_\\theta(z_t;t)\n",
        "\\end{align} $$\n",
        "\n",
        "This shows that we can think about this thing and parameterize it in terms of three different things, either the denoised image $\\hat x$, or the noise $\\hat \\epsilon$ or the scores.  The relationship between the denoised image and the noise itself is:\n",
        "\n",
        "$$  \\hat x_\\theta(z_t, t) = (z_t - \\sigma_t \\hat \\epsilon_\\theta(z_t; t) ) / \\alpha_t $$\n",
        "\n",
        "This is the actual learned component of our model here.  We are attempting to learn a model that at any intermediate stage of corruption allows us to predict what the uncorrupted image was.  We can either do that directly $\\hat x_\\theta$, or equivalent to saying what the uncorrupted image is, we can spit out the corruption as in $\\hat \\epsilon_\\theta$.  Finally, as is shown in the VDM paper in appendix L, we could also connect back to the denoising diffusion type models and think about the task as learning the score model $s_\\theta$ for the marginal of the forward process:\n",
        "$$ s_\\theta(z_t) = \\nabla_{z_t} \\log q(z_t)  $$"
      ],
      "metadata": {
        "id": "V1RCDEOi6t8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To actually learn this model, we need to build out the parametric model architecture. In the VDM paper they use a type of U-Net.  We need to take in a perturbed image and try to predict the noise that is in that image.  This means we have an image sized input and want to produce an image sized output.  Another important part is that we want to have the thing conditioned on the time step we are on so we can use the same architecture for the whole of the process."
      ],
      "metadata": {
        "id": "z4Xb30cThL--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "8ziLVaK7hKNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With most of our pieces in place, now we need to stick them all together and look at the form of our loss.\n",
        "\n",
        "At a high level, our loss is quite simple in form, we have a forward and a reverse process and our loss is simply the KL divergence of the forward to the reverse process:\n",
        "\n",
        "$$ \\mathcal J = \\left\\langle \\log \\frac{q(x, z_0,z_1,\\cdots,z_T)}{p(x, z_0,z_1,\\cdots,z_T)} \\right\\rangle_q \\geq \\left\\langle \\log \\frac{q(x)}{p(x)} \\right\\rangle_q \\geq 0 $$\n",
        "$q$ is our forward process which starts with an image and does successive steps of gaussian diffusion to it.  The reverse process starts from an isotropic gaussian and then runs our reverse process.  KL divergence is non-negative, and its also *monotonic* in the sense that any transformation of the random variables will reduce the KL between two distributions.  One form of transformation we could do is marginalization.  So we know that if we look at the KL of the $N$ step forward and reverse processes, that will be an upper bound on the KL divergence between the marginal $x$ distributions, $q(x)$ being the image distribution we start our forward samples from (the thing we're trying to learn to approximate) and $p(x)$ the marginal of running our reverse process.  So without any real work, we're assured that by minimizing the KLs between the full joints of the forward and reverse process the reverse process itself is going to learn to approximate the true image distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "9rvz1abdcyOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get this into a recognizeable and computable form, we're going to do the kind of rearrangement we considered above, so, while its most natural and we discussed the forward process as being a sequence of steps:\n",
        "\n",
        "$$ q(x,z_0,z_1,z_2,\\cdots,z_T) = q(x) q(z_0|x) q(z_1|z_0) q(z_2|z_1) \\cdots q(z_T|z_{T-1}) $$\n",
        "we can just as well imagine a different factorization that leaves the conditioning on the first step explicit:\n",
        "$$\n",
        "\\begin{align}\n",
        "q(x, z_0, z_1, z_2,\\cdots, z_N) &= q(z_0,z_1,z_2,\\cdots, z_T|x) q(x) \\\\\n",
        "&= q(z_0|z_1,\\cdots,z_T,x)q(z_1|z_2,\\cdots,z_T,x)q(z_T|x)q(x) \\\\\n",
        "&= q(z_0|z_1,x)q(z_1|z_2,x)\\cdots q(z_{T-1}|z_{T},x)q(z_T|x)p(x)\n",
        "\\end{align}$$\n",
        "\n",
        "In the other direction we consider the natural factorization of the reverse process:\n",
        "$$ p(x,z_0,z_1,\\cdots,z_T) = p(z_T) p(z_{T-1}|z_T) \\cdots p(z_0|z_1) p(x|z_0) $$\n",
        "\n",
        "With these factorizations in place we can now look at the full KLs and organize terms.\n",
        "\n",
        "$$\n",
        "  \\left\\langle \\log q(x) - \\log p(x|z_0) + \\log \\frac{q(z_T|x)}{p(z_T)}  \n",
        "  + \\sum_{i=0}^{T-1} \\log \\frac{q(z_i|z_{i+1},x)}{p(z_i|z_{i+1})}\n",
        "  \\right\\rangle_q\n",
        "$$\n",
        "\n",
        "The last trick we're going to use is that we're going to avoid computing all of the terms in our sum by simply not computing all of the terms in our sum.  We'll approximate the sum with monte carlo, we'll simply randomly choose one of the terms and upweight it by the number of terms in the sum."
      ],
      "metadata": {
        "id": "v3nusddz6mgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "Now we are ready to build out the architecture for our model.  First we want to be able to embed our timestep to a higher dimensional signal that the model can use to do some interesting conditioning.  For that we'll use some fourier feature inspired embeddings."
      ],
      "metadata": {
        "id": "FvUd1uQoB_3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_timestep_embedding(timesteps, embedding_dim: int, dtype=np.float32):\n",
        "  \"\"\"Build sinusoidal embeddings (from Fairseq).\"\"\"\n",
        "\n",
        "  assert len(timesteps.shape) == 1\n",
        "  timesteps *= 1000\n",
        "\n",
        "  half_dim = embedding_dim // 2\n",
        "  emb = np.log(10_000) / (half_dim - 1)\n",
        "  emb = np.exp(np.arange(half_dim, dtype=dtype) * -emb)\n",
        "  emb = timesteps.astype(dtype)[:, None] * emb[None, :]\n",
        "  emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n",
        "  if embedding_dim %2 == 1: # zero pad\n",
        "    emb = jax.lax.pad(emb, dtype(0), ((0, 0, 0), (0, 1, 0)))\n",
        "  assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
        "  return emb"
      ],
      "metadata": {
        "id": "O5rq6xovwhgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next as a sort of primitive, for this work we'll be using a simple resnet style architecture for the components."
      ],
      "metadata": {
        "id": "qWxMgpbFEYoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  hidden_size: int = 512\n",
        "  n_layers: int = 1\n",
        "  middle_size: int = 1024\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, cond=None):\n",
        "    assert x.shape[-1] == self.hidden_size, \"Input must be hidden size.\"\n",
        "    z = x\n",
        "    for i in range(self.n_layers):\n",
        "      h = nn.gelu(nn.LayerNorm()(z))\n",
        "      h = nn.Dense(self.middle_size)(h)\n",
        "      if cond is not None:\n",
        "        h += nn.Dense(self.middle_size, use_bias=False)(cond)\n",
        "      h = nn.gelu(nn.LayerNorm()(h))\n",
        "      h = nn.Dense(self.hidden_size, kernel_init=jax.nn.initializers.zeros)(h)\n",
        "      z = z + h\n",
        "    return z\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aSUTpFYUpEhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are doing latent diffusion here, we need an initial encoder to map our images to some lower dimensional space, here we use a resnet to accomplish this."
      ],
      "metadata": {
        "id": "Byu0pHTcEd0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "realize = tfb.Shift(-1)(tfb.Scale(2)(tfb.Scale(1./256)(tfb.Shift(0.5))))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  hidden_size: int = 256\n",
        "  n_layers: int = 3\n",
        "  z_dim: int = 128\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, ims, cond=None):\n",
        "    x = 2 * ims.astype('float32') - 1.0\n",
        "    x = einops.rearrange(x, '... x y d -> ... (x y d)')\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = ResNet(self.hidden_size, self.n_layers)(x, cond=cond)\n",
        "    params = nn.Dense(self.z_dim)(x)\n",
        "    return params\n",
        ""
      ],
      "metadata": {
        "id": "EdeEWvOFqWp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then at the end of our chain, we need a decoder to map the latent representation to a full sized image again.  For this we'll use another small resnet and use a mean field bernoulli observiational model.  While this normally might be a poor approximation, in this colab we are doing dynamic binarization of the emnist digits, so this is a good model for the final observed noise in the data."
      ],
      "metadata": {
        "id": "zNW9jbHSEkcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  hidden_size: int = 512\n",
        "  n_layers: int = 3\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, z, cond=None):\n",
        "    z = nn.Dense(self.hidden_size)(z)\n",
        "    z = ResNet(self.hidden_size, self.n_layers)(z, cond=cond)\n",
        "    logits = nn.Dense(28 * 28 * 1)(z)\n",
        "    logits = einops.rearrange(logits, '... (x y d) -> ... x y d', x=28, y=28, d=1)\n",
        "    return tfd.Independent(tfd.Bernoulli(logits=logits), 3)\n"
      ],
      "metadata": {
        "id": "mAtZpgW2q0a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the real meat and potatoes of our diffusion model is the scorenet, a model that given a noisy image tries to predict the noise that is contained in that image.  It tries to clean up the image but by prediciting what you should remove.\n",
        "\n",
        "This architecture needs to map a latent sized object to another latent sized object, so we'll use another resnet, but be sure to feed in our conditioning signal in a FiLM style."
      ],
      "metadata": {
        "id": "cF48A8nEEz9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoreNet(nn.Module):\n",
        "  embedding_dim: int = 128\n",
        "  n_layers: int = 10\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, z, g_t, conditioning):\n",
        "    n_embd = self.embedding_dim\n",
        "\n",
        "    t = g_t\n",
        "    assert np.isscalar(t) or len(t.shape) == 0 or len(t.shape) == 1\n",
        "    t = t * np.ones(z.shape[0])  # ensure t is a vector\n",
        "\n",
        "    temb = get_timestep_embedding(t, n_embd)\n",
        "    cond = np.concatenate([temb, conditioning], axis=1)\n",
        "    cond = nn.swish(nn.Dense(features=n_embd * 4, name='dense0')(cond))\n",
        "    cond = nn.swish(nn.Dense(features=n_embd * 4, name='dense1')(cond))\n",
        "    cond = nn.Dense(n_embd)(cond)\n",
        "\n",
        "    h = nn.Dense(n_embd)(z)\n",
        "    h = ResNet(n_embd, self.n_layers)(h, cond)\n",
        "    return z + h\n"
      ],
      "metadata": {
        "id": "x7KSXrXz3yz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the components in place we can build our our actual VDM model.  The main callable here computes an unbiased estimate of our VDM loss."
      ],
      "metadata": {
        "id": "PZJNu_3eFM9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDM(nn.Module):\n",
        "  timesteps: int = 1000\n",
        "  gamma_min: float = -3.0 # -13.3\n",
        "  gamma_max: float = 3.0  # 5.0\n",
        "  embedding_dim: int = 256\n",
        "  antithetic_time_sampling: bool = True\n",
        "  layers: int = 32\n",
        "  classes: int = 10 + 26 + 26 + 1\n",
        "\n",
        "  warmup_epochs: int = 3\n",
        "\n",
        "  def detect_anomalies(self, params, batch, rng):\n",
        "    im, lb = batch\n",
        "    l1, l2, l3 = self.apply(params, im.astype('float'), lb, rngs={\"sample\": rng})\n",
        "    total_loss = l1 + l2 + l3\n",
        "\n",
        "    # loss 값들의 분포 분석\n",
        "    losses = np.array(total_loss)\n",
        "    sorted_losses = np.sort(losses)\n",
        "    gaps = np.diff(sorted_losses)\n",
        "    mean_gap = np.mean(gaps)\n",
        "    std_gap = np.std(gaps)\n",
        "    threshold = mean_gap + 2 * std_gap\n",
        "\n",
        "    # 이상치 인덱스 반환\n",
        "    return np.where(total_loss > threshold)[0]\n",
        "\n",
        "  def setup(self):\n",
        "    self.gamma = partial(gamma, gamma_min=self.gamma_min, gamma_max=self.gamma_max)\n",
        "    self.score_model = ScoreNet(n_layers=self.layers,\n",
        "                                embedding_dim=self.embedding_dim)\n",
        "    self.encoder = Encoder(z_dim=self.embedding_dim)\n",
        "    self.decoder = Decoder()\n",
        "    self.embedding_vectors = nn.Embed(self.classes, self.embedding_dim)\n",
        "\n",
        "  def gammat(self, t):\n",
        "    return self.gamma(t)\n",
        "\n",
        "  def recon_loss(self, x, f, cond):\n",
        "    \"\"\"The reconstruction loss measures the gap in the first step.\n",
        "\n",
        "    We measure the gap from encoding the image to z_0 and back again.\"\"\"\n",
        "    # ## Reconsturction loss 2\n",
        "    g_0 = self.gamma(0.0)\n",
        "    eps_0 = random.normal(self.make_rng(\"sample\"), shape=f.shape)\n",
        "    z_0 = variance_preserving_map(f, g_0, eps_0)\n",
        "    z_0_rescaled = z_0 / alpha(g_0)\n",
        "    loss_recon = -self.decoder(z_0_rescaled, cond).log_prob(x.astype('int32'))\n",
        "    return loss_recon\n",
        "\n",
        "  def latent_loss(self, f):\n",
        "    \"\"\"The latent loss measures the gap in the last step, this is the KL\n",
        "    divergence between the final sample from the forward process and starting\n",
        "    distribution for the reverse process, here taken to be a N(0,1).\"\"\"\n",
        "    # KL z1 with N(0,1) prior\n",
        "    g_1 = self.gamma(1.0)\n",
        "    var_1 = sigma2(g_1)\n",
        "    mean1_sqr = (1. - var_1) * np.square(f)\n",
        "    loss_klz = 0.5 * np.sum(mean1_sqr + var_1 - np.log(var_1) - 1., axis=-1)\n",
        "    return loss_klz\n",
        "\n",
        "  def diffusion_loss(self, t, f, cond):\n",
        "    # sample z_t\n",
        "    g_t = self.gamma(t)\n",
        "    eps = jax.random.normal(self.make_rng(\"sample\"), shape=f.shape)\n",
        "    z_t = variance_preserving_map(f, g_t[:, None], eps)\n",
        "    # compute predicted noise\n",
        "    eps_hat = self.score_model(z_t, g_t, cond)\n",
        "    # compute MSE of predicted noise\n",
        "    loss_diff_mse = np.sum(np.square(eps - eps_hat), axis=-1)\n",
        "\n",
        "    # loss for finite depth T, i.e. discrete time\n",
        "    T = self.timesteps\n",
        "    s = t - (1./T)\n",
        "    g_s = self.gamma(s)\n",
        "    loss_diff = .5 * T * np.expm1(g_s - g_t) * loss_diff_mse\n",
        "    return loss_diff\n",
        "\n",
        "  def __call__(self, images, conditioning,\n",
        "               sample_shape=()):\n",
        "\n",
        "\n",
        "    x = images\n",
        "    n_batch = images.shape[0]\n",
        "\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "\n",
        "    # 1. RECONSTRUCTION LOSS\n",
        "    # add noise and reconstruct\n",
        "    f = self.encoder(x, cond)\n",
        "    loss_recon = self.recon_loss(x, f, cond)\n",
        "\n",
        "    # 2. LATENT LOSS\n",
        "    # KL z1 with N(0,1) prior\n",
        "    loss_klz = self.latent_loss(f)\n",
        "\n",
        "    # 3. DIFFUSION LOSS\n",
        "    # sample time steps\n",
        "    rng1 = self.make_rng(\"sample\")\n",
        "    if self.antithetic_time_sampling:\n",
        "      t0 = jax.random.uniform(rng1)\n",
        "      t = np.mod(t0 + np.arange(0., 1., step=1. / n_batch), 1.0)\n",
        "    else:\n",
        "      t = jax.random.uniform(rng1, shape=(n_batch,))\n",
        "\n",
        "    # discretize time steps if we're working with discrete time\n",
        "    T = self.timesteps\n",
        "    t = np.ceil(t * T) / T\n",
        "\n",
        "    loss_diff = self.diffusion_loss(t, f, cond)\n",
        "\n",
        "    # End of diffusion loss computation\n",
        "    return (loss_diff, loss_klz, loss_recon)\n",
        "\n",
        "  def embed(self, conditioning):\n",
        "    return self.embedding_vectors(conditioning)\n",
        "\n",
        "  def encode(self, ims, conditioning=None):\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "    return self.encoder(ims, cond)\n",
        "\n",
        "  def decode(self, z0, conditioning=None):\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "    return self.decoder(z0, cond)\n",
        "\n",
        "  def shortcut(self, ims, conditioning=None):\n",
        "    \"Evaluates the performance of the encoder / decoder by itself.\"\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "    f = self.encoder(ims, cond)\n",
        "    eps_0 = random.normal(self.make_rng(\"sample\"), shape=f.shape)\n",
        "    g_0 = self.gamma(0.)\n",
        "    z_0 = variance_preserving_map(f, g_0, eps_0)\n",
        "    z_0_rescaled = z_0 / alpha(g_0)\n",
        "    return self.decoder(z_0_rescaled, cond)\n",
        "\n",
        "\n",
        "  def sample_step(self, rng, i, T, z_t, conditioning, guidance_weight=0.):\n",
        "    rng_body = jax.random.fold_in(rng, i)\n",
        "    eps = random.normal(rng_body, z_t.shape)\n",
        "    t = (T - i)/T\n",
        "    s = (T - i - 1) / T\n",
        "\n",
        "    g_s = self.gamma(s)\n",
        "    g_t = self.gamma(t)\n",
        "\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "\n",
        "    eps_hat_cond = self.score_model(\n",
        "        z_t,\n",
        "        g_t * np.ones((z_t.shape[0],), z_t.dtype),\n",
        "        cond,)\n",
        "\n",
        "    eps_hat_uncond = self.score_model(\n",
        "        z_t,\n",
        "        g_t * np.ones((z_t.shape[0],), z_t.dtype),\n",
        "        cond * 0.,)\n",
        "    eps_hat = (1. + guidance_weight) * eps_hat_cond - guidance_weight * eps_hat_uncond\n",
        "\n",
        "\n",
        "    a = nn.sigmoid(g_s)\n",
        "    b = nn.sigmoid(g_t)\n",
        "    c = -np.expm1(g_t - g_s)\n",
        "    sigma_t = np.sqrt(sigma2(g_t))\n",
        "    z_s = np.sqrt(a / b) * (z_t - sigma_t * c * eps_hat) + np.sqrt((1. - a) * c) * eps\n",
        "    return z_s\n",
        "\n",
        "\n",
        "  def recon(self, z, t, conditioning):\n",
        "    g_t = self.gamma(t)[:, None]\n",
        "    cond = self.embedding_vectors(conditioning)\n",
        "    eps_hat = self.score_model(z, g_t, cond)\n",
        "    sigmat = np.sqrt(sigma2(g_t))\n",
        "    alphat = np.sqrt(1 - sigmat**2)\n",
        "    xhat = (z - sigmat * eps_hat ) / alphat\n",
        "    return (eps_hat, xhat)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EdOgPX14h839"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model defined, we'll define a few helper functions to let us do generations, reconstructions or likelihood calculations with our model."
      ],
      "metadata": {
        "id": "6IZts_DSFY9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(vdm, params, rng, shape, conditioning, guidance_weight=0.0):\n",
        "  # first generate latent\n",
        "  rng, spl = random.split(rng)\n",
        "  zt = random.normal(spl, shape + (vdm.embedding_dim,))\n",
        "\n",
        "  def body_fn(i, z_t):\n",
        "    return vdm.apply(\n",
        "        params,\n",
        "        rng,\n",
        "        i,\n",
        "        vdm.timesteps,\n",
        "        z_t,\n",
        "        conditioning,\n",
        "        guidance_weight=guidance_weight,\n",
        "        method=vdm.sample_step)\n",
        "\n",
        "  z0 = jax.lax.fori_loop(\n",
        "      lower=0, upper=vdm.timesteps, body_fun=body_fn, init_val=zt)\n",
        "  g0 = vdm.apply(params, 0.0, method=vdm.gammat)\n",
        "  var0 = sigma2(g0)\n",
        "  z0_rescaled = z0 / np.sqrt(1. - var0)\n",
        "  return vdm.apply(params, z0_rescaled, conditioning, method=vdm.decode)\n",
        "\n",
        "\n",
        "def recon(vdm, params, rng, t, ims, conditioning):\n",
        "  # first generate latent\n",
        "  rng, spl = random.split(rng)\n",
        "  z_0 = vdm.apply(params, ims, conditioning, method=vdm.encode)\n",
        "\n",
        "  T = vdm.timesteps\n",
        "  tn = np.ceil(t * T)\n",
        "  t = tn / T\n",
        "  g_t = vdm.apply(params, t, method=vdm.gammat)\n",
        "  rng, spl = random.split(rng)\n",
        "  eps = jax.random.normal(spl, shape=z_0.shape)\n",
        "  z_t = variance_preserving_map(z_0, g_t[:,None], eps)\n",
        "\n",
        "  def body_fn(i, z_t):\n",
        "    return vdm.apply(\n",
        "        params,\n",
        "        rng,\n",
        "        i,\n",
        "        vdm.timesteps,\n",
        "        z_t,\n",
        "        conditioning,\n",
        "        method=vdm.sample_step)\n",
        "\n",
        "  z0 = jax.lax.fori_loop(\n",
        "      lower=(T - tn).astype('int'),\n",
        "      upper=vdm.timesteps, body_fun=body_fn, init_val=z_t)\n",
        "  g0 = vdm.apply(params, 0.0, method=vdm.gammat)\n",
        "  var0 = sigma2(g0)\n",
        "  z0_rescaled = z0 / np.sqrt(1. - var0)\n",
        "  return vdm.apply(params, z0_rescaled, conditioning, method=vdm.decode)"
      ],
      "metadata": {
        "id": "3kxhUta-FYV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elbo(vdm, params, rng, ims, conditioning):\n",
        "  x = ims\n",
        "  rng, spl = jax.random.split(rng)\n",
        "  cond = vdm.apply(params, conditioning, method=vdm.embed)\n",
        "  f = vdm.apply(params, ims, conditioning, method=vdm.encode)\n",
        "  loss_recon = vdm.apply(params, x, f, cond, rngs={\"sample\": rng}, method=vdm.recon_loss)\n",
        "  loss_klz = vdm.apply(params, f, method=vdm.latent_loss)\n",
        "\n",
        "  def body_fun(i, val):\n",
        "    loss, rng = val\n",
        "    rng, spl = jax.random.split(rng)\n",
        "    new_loss = vdm.apply(params, np.array([i / vdm.timesteps]), f, cond, rngs={\"sample\": spl}, method=vdm.diffusion_loss)\n",
        "    return (loss + new_loss / vdm.timesteps, rng)\n",
        "\n",
        "  loss_diff, rng = jax.lax.fori_loop(\n",
        "      0, vdm.timesteps, body_fun, (np.zeros(ims.shape[0]), rng))\n",
        "\n",
        "  return loss_recon + loss_klz + loss_diff"
      ],
      "metadata": {
        "id": "o8hbibDDmp6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "op9qH1lOCEGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all the components in place, its time to train our model"
      ],
      "metadata": {
        "id": "AqQrT0DyFjx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vdm = VDM(gamma_min=-5.0,\n",
        "          gamma_max=1.0,\n",
        "          layers=4,\n",
        "          embedding_dim=32,\n",
        "          timesteps=1000)\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "batch = next(create_input_iter(train_ds))[0][0]\n",
        "conditioning = np.zeros(batch.shape[0], dtype='int')\n",
        "out, params = vdm.init_with_output({\"sample\": rng, \"params\": rng}, batch, conditioning)\n",
        "print(f\"Params: {param_count(params):,}\")"
      ],
      "metadata": {
        "id": "Iv7Tj-41S2wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(params, rng, im, lb, beta=1.0):\n",
        "  l1, l2, l3 = vdm.apply(params, im.astype('float'), lb, rngs={\"sample\": rng})\n",
        "  rescale_to_bpd = 1./(onp.prod(im.shape[1:]) * np.log(2.0))\n",
        "  return (l1.mean()/beta + l2.mean()/beta + l3.mean()) * rescale_to_bpd"
      ],
      "metadata": {
        "id": "BV3eMM_jkpZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class Store:\n",
        "  params: np.ndarray\n",
        "  state: Any\n",
        "  rng: Any\n",
        "  step: int = 0\n",
        "\n",
        "\n",
        "TSTEPS = 20_000 // jax.device_count()\n",
        "\n",
        "# we'll use adamw with some linear warmup and a cosine decay.\n",
        "opt = optax.chain(\n",
        "    optax.scale_by_schedule(optax.cosine_decay_schedule(1.0, TSTEPS, 1e-5)),\n",
        "    optax.adamw(8e-4, b1=0.9, b2=0.99, eps=1e-8, weight_decay=1e-4),\n",
        "    optax.scale_by_schedule(\n",
        "        optax.linear_schedule(0.0, 1.0, 250)))\n",
        "\n",
        "store = Store(params, opt.init(params), rng, 0)\n",
        "pstore = replicate(store)"
      ],
      "metadata": {
        "id": "un7JiolBGQ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll make a simple step function that we can pmap to make better use of our accelerators."
      ],
      "metadata": {
        "id": "Om3miOkRFvIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "\n",
        "def remove_anomalies(batch, anomaly_indices):\n",
        "   im, lb = batch\n",
        "   mask = np.ones(im.shape[0], dtype=bool)\n",
        "   mask = mask.at[anomaly_indices].set(False)\n",
        "   return (im[mask], lb[mask])\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='batch')\n",
        "def step(store, batch):\n",
        "    rng, spl = random.split(store.rng)\n",
        "    im, lb = batch\n",
        "\n",
        "    warmup_steps = vdm.warmup_epochs * (TSTEPS // vdm.warmup_epochs)\n",
        "\n",
        "    # loss 계산\n",
        "    l1, l2, l3 = vdm.apply(store.params, im.astype('float'), lb, rngs={\"sample\": spl})\n",
        "    total_loss = l1 + l2 + l3\n",
        "\n",
        "    # 이상치 검출을 위한 threshold 계산\n",
        "    mean_loss = jax.lax.pmean(np.mean(total_loss), 'batch')\n",
        "    std_loss = jax.lax.pmean(np.std(total_loss), 'batch')\n",
        "    threshold = mean_loss + 2 * std_loss\n",
        "\n",
        "    # 마스크 생성\n",
        "    mask = total_loss <= threshold\n",
        "\n",
        "    # mask를 사용하여 배치 필터링\n",
        "    def filter_batch(x):\n",
        "        return x * mask[:, None, None, None].astype(x.dtype)  # 타입 일치시킴\n",
        "\n",
        "    im = jax.lax.cond(\n",
        "        store.step >= warmup_steps,\n",
        "        lambda x: filter_batch(x),\n",
        "        lambda x: x,\n",
        "        im\n",
        "    )\n",
        "\n",
        "    # 일반적인 학습 단계\n",
        "    out, grads = jax.value_and_grad(loss)(store.params, spl, im, lb)\n",
        "    grads = jax.lax.pmean(grads, 'batch')\n",
        "    updates, state = opt.update(grads, store.state, store.params)\n",
        "    params = optax.apply_updates(store.params, updates)\n",
        "\n",
        "    return (store.replace(\n",
        "        params=params,\n",
        "        state=state,\n",
        "        rng=rng,\n",
        "        step=store.step + 1),\n",
        "        jax.lax.pmean(out, 'batch'))"
      ],
      "metadata": {
        "id": "x7RSEMMbGQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals = []"
      ],
      "metadata": {
        "id": "oWDHkfeWKWUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our actual training, with the standard configuration in this colab, and using a GPU as an accelerator this should take about ~30 minutes on a T4."
      ],
      "metadata": {
        "id": "fpqZmpxBF17X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = create_input_iter(train_ds)\n",
        "ebatches = create_input_iter(test_ds)\n",
        "\n",
        "with trange(TSTEPS) as t:\n",
        "    for i in t:\n",
        "        if i % (TSTEPS // vdm.warmup_epochs) == 0:\n",
        "            batches = create_input_iter(train_ds)\n",
        "            print(f\"Starting epoch {i // (TSTEPS // vdm.warmup_epochs) + 1}\")\n",
        "\n",
        "        pstore, val = step(pstore, next(batches))\n",
        "        v = unreplicate(val)\n",
        "        t.set_postfix(val=v)\n",
        "        vals.append(v)"
      ],
      "metadata": {
        "id": "zPe0ZfgDG1ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our loss went down nicely over time."
      ],
      "metadata": {
        "id": "WExhh0x0F9iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vals[1000:])"
      ],
      "metadata": {
        "id": "dLnZgT9Ivkht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Having trained the model we can now investigate its performance."
      ],
      "metadata": {
        "id": "nuf7ZHq0sLbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss\n",
        "\n",
        "We can evaluate the loss of the model on the test set (here just a single batch but can be extended).\n",
        "\n",
        "We can evaluate both the conditional likelihood:"
      ],
      "metadata": {
        "id": "61Sp4FxTsZ1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = unreplicate(next(ebatches))\n",
        "ims, lbs = batch\n",
        "loss_diff, loss_klz, loss_recon = vdm.apply(unreplicate(pstore.params), ims, lbs, rngs={\"sample\": rng})\n",
        "losses = jax.tree_map(lambda x: np.mean(x) / (onp.prod(ims.shape[1:]) * np.log(2)), {\"loss_diff\": loss_diff, \"loss_klz\": loss_klz, \"loss_recon\": loss_recon})\n",
        "print(losses, \"\\n\", sum(losses.values()))"
      ],
      "metadata": {
        "id": "z5mI6TMvBxGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as the unconditional likelihood"
      ],
      "metadata": {
        "id": "ZTf52DggstRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_diff, loss_klz, loss_recon = vdm.apply(unreplicate(pstore.params), ims, 0 * lbs, rngs={\"sample\": rng})\n",
        "losses = jax.tree_map(lambda x: np.mean(x) / (onp.prod(ims.shape[1:]) * np.log(2)), {\"loss_diff\": loss_diff, \"loss_klz\": loss_klz, \"loss_recon\": loss_recon})\n",
        "print(losses, \"\\n\", sum(losses.values()))"
      ],
      "metadata": {
        "id": "NhDObIGp7V6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also generate images from the model"
      ],
      "metadata": {
        "id": "kj3jKHNqsvxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viewer(x, **kwargs):\n",
        "  # x = np.clip((x + 1)/2.0, 0, 1)\n",
        "  return rawarrview(reshape_image_batch(x), **kwargs)"
      ],
      "metadata": {
        "id": "uQQnb8_XX4dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll demonstrate unconditional generations from the model, as well as class conditional generations.  Finally we'll also showcase some guided generations which use [classifier free guidance](https://arxiv.org/abs/2207.12598) to clean up the samples."
      ],
      "metadata": {
        "id": "3muIwaXMsyVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditioning = np.arange(128) % (10 + 26 + 26 + 1)\n",
        "gen_dist = generate(vdm, unreplicate(pstore).params, rng, (128,), conditioning)\n",
        "gen = gen_dist.mean()\n",
        "ugen_dist = generate(vdm, unreplicate(pstore).params, rng, (128,), 0 *conditioning)\n",
        "ugen = ugen_dist.mean()\n",
        "cgen_dist = generate(vdm, unreplicate(pstore).params, rng, (128,), conditioning, guidance_weight=1.0)\n",
        "cgen = cgen_dist.mean()\n",
        "print(\"UNCOND GENERATIONS\")\n",
        "viewer(ugen.squeeze(-1), cmap='bone_r')\n",
        "print(\"CONDITIONAL GENERATIONS\")\n",
        "viewer(gen.squeeze(-1), cmap='bone_r')\n",
        "print(\"CONDITIONAL GENERATIONS with CLASSIFIER GUIDANCE\")\n",
        "viewer(cgen.squeeze(-1), cmap='bone_r')"
      ],
      "metadata": {
        "id": "bUmgU2xU9kwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the impact of the guidance weight on the quality of the samples. Below each row corresponds to samples with a different guidance weight. A guidance weight of -1 is unconditional samples, 0 is conditional samples, and values > 0 are increasing classifier-free guidance."
      ],
      "metadata": {
        "id": "8OEYjK7txfB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "gws = (-1., 0., 1., 2., 4., 8.)\n",
        "for guidance_weight in gws:\n",
        "  cgen_dist = generate(vdm, unreplicate(pstore).params, rng, (128,),\n",
        "                       conditioning, guidance_weight=guidance_weight)\n",
        "  samples.append(cgen_dist.mean().squeeze(-1))\n",
        "\n",
        "n = len(gws)\n",
        "plt.figure(figsize=(20,4))\n",
        "for i, (gw, imgs) in enumerate(zip(gws, samples)):\n",
        "  plt.subplot(n, 1, i + 1)\n",
        "  plt.imshow(reshape_image_batch(imgs, rows=1, cut=28 + 10), cmap='bone_r')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.box(False)\n",
        "  plt.ylabel('%.f'%gw)"
      ],
      "metadata": {
        "id": "Qr_aH1XixsBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model is a conditional generative model, another thing we can do is use it as a classifier.  Here we'll evaluate its performance on a single test batch."
      ],
      "metadata": {
        "id": "oytT4fx-tAoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditioning = np.zeros(512).astype('int')\n",
        "rng, spl = jax.random.split(rng)\n",
        "elbos = jax.jit(lambda pk: elbo(vdm, unreplicate(pstore).params, spl, ims, (np.zeros(ims.shape[0]) + pk).astype('int')))\n",
        "cond_elbos = jax.lax.map(elbos, np.arange(26 + 26 + 10 + 1))\n",
        "(jax.nn.softmax(-cond_elbos.at[0].set(np.inf), axis=0).argmax(0) == lbs).mean()"
      ],
      "metadata": {
        "id": "VC4koO4irblH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll look at some \"reconstructions\" in our diffusion model where we encode some images to some finite depth in our model and then run the generative process on the way back."
      ],
      "metadata": {
        "id": "fAuvgugDtJe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 0.8\n",
        "recon_dist = recon(vdm, unreplicate(pstore).params, rng,\n",
        "                   np.array([T]), ims, lbs)\n",
        "rec = recon_dist.mean()\n",
        "urecon_dist = recon(vdm, unreplicate(pstore).params, rng,\n",
        "                   np.array([T]), ims, 0 * lbs)\n",
        "urec = urecon_dist.mean()\n",
        "\n",
        "viewer(ims.squeeze(-1), cmap='bone_r')\n",
        "print(\"RECON\")\n",
        "viewer(rec.squeeze(-1), cmap='bone_r')\n",
        "print(\"UNCOND RECON\")\n",
        "viewer(urec.squeeze(-1), cmap='bone_r')"
      ],
      "metadata": {
        "id": "GMexhVe2L_-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our diffusion model was a latent diffusion model, so it had its own encoder and decoder as part of the model, here we'll look at the reconstructions in that model, i.e. we'll do a diffusion style reconstruction but at t=0."
      ],
      "metadata": {
        "id": "r9NKVNDJtmye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recon_dist = vdm.apply(unreplicate(pstore).params, ims, lbs, method=vdm.shortcut, rngs={\"sample\": rng})\n",
        "rec = recon_dist.mean()\n",
        "urecon_dist = vdm.apply(unreplicate(pstore).params, ims, 0 * lbs, method=vdm.shortcut, rngs={\"sample\": rng})\n",
        "urec = urecon_dist.mean()\n",
        "\n",
        "viewer(ims.squeeze(-1), cmap='bone_r')\n",
        "print(\"SHORTCUT\")\n",
        "viewer(rec.squeeze(-1), cmap='bone_r')\n",
        "print(\"UNCOND SHORTCUT\")\n",
        "viewer(urec.squeeze(-1), cmap='bone_r')"
      ],
      "metadata": {
        "id": "xgunpxzU9kpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p44QLzuCkiLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}